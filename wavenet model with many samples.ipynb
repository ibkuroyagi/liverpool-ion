{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import os\n",
    "from wavenet import * \n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations and main hyperparammeters\n",
    "EPOCHS = 150\n",
    "NNBATCHSIZE = 16\n",
    "GROUP_BATCH_SIZE = 4000\n",
    "look_back = 1024\n",
    "SEED = 321\n",
    "LR = 0.001\n",
    "SPLITS = 5\n",
    "sample_batch = 10\n",
    "sample_size = 500000\n",
    "\n",
    "outdir = 'models'\n",
    "flip = False\n",
    "noise = False\n",
    "\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "def read_data():\n",
    "    train = pd.read_csv('input/train.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
    "    test  = pd.read_csv('input/test.csv', dtype={'time': np.float32, 'signal': np.float32})\n",
    "    sub  = pd.read_csv('input/sample_submission.csv', dtype={'time': np.float32})\n",
    "    return train, test, sub\n",
    "\n",
    "# create batches of 4000 observations\n",
    "def batching(df, batch_size):\n",
    "    #print(df)\n",
    "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
    "    df['group'] = df['group'].astype(np.uint16)\n",
    "    return df\n",
    "\n",
    "# normalize the data (standard scaler). We can also try other scalers for a better score!\n",
    "def normalize(train, test):\n",
    "    train_input_mean = train.signal.mean()\n",
    "    train_input_sigma = train.signal.std()\n",
    "    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n",
    "    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n",
    "    train['signal'] = (((train.signal - train.signal.min()) / (train.signal.max()-train.signal.min()))-0.5)*2\n",
    "    test['signal'] = (((test.signal - test.signal.min()) / (test.signal.max()-test.signal.min()))-0.5)*2\n",
    "    return train, test\n",
    "\n",
    "train, test, sample_submission = read_data()\n",
    "train, test = normalize(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class subIronDataset(Dataset):\n",
    "    def __init__(self, data, labels_df=None, training=True, index=None, mu=256, look_back=1024):\n",
    "        self.data = data\n",
    "        if training:\n",
    "            #self.labels = pd.get_dummies(labels_df)\n",
    "            self.labels = labels_df\n",
    "        if index is not None:\n",
    "            self.data = data.iloc[index]\n",
    "            \n",
    "        self.training = training\n",
    "        self.index = index\n",
    "        self.mu = mu\n",
    "        self.len_sample = 500000 #１回の実験でサンプリングしたデータの点\n",
    "        self.look_back = look_back\n",
    "        self.class_num = 11\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.index is not None:\n",
    "            return len(self.index)\n",
    "        else:\n",
    "            return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.index is not None:\n",
    "            idx = self.index[idx]\n",
    "        data = np.array([encode_mu_law(self.data[idx], mu=self.mu)])\n",
    "        #labelを返す際にlook_back分のクラスを付与 label=(batch, look_back, class_num)\n",
    "        #ただし、実装の簡単のためidxにはintが来る(dataloader)での想定とするlabel=(look_back, class_num)\n",
    "        #look_backの部分は[now-look_back+1:now+1]という順番で入っている\n",
    "        if self.training:\n",
    "            if idx%self.len_sample <= self.look_back-1: #look_back未満の部分でmodeで保管する(もし精度が悪ければ変える)\n",
    "                labels = np.zeros(self.look_back)\n",
    "                new_idx = idx%self.len_sample\n",
    "                num_batch = idx//self.len_sample\n",
    "                labels[self.look_back-new_idx-1:] = self.labels.iloc[self.len_sample*num_batch:idx+1].values\n",
    "                labels[:self.look_back-new_idx-1] = labels[self.look_back-new_idx-1:].sum(axis=0).argmax()\n",
    "                #one_hot = np.zeros(self.class_num)\n",
    "                #print(labels[self.look_back-idx-1:].sum(axis=0).argmax())\n",
    "                #one_hot[labels[self.look_back-idx-1:].sum(axis=0).argmax()] = 1 #mode\n",
    "                #print(one_hot)\n",
    "                #labels[:self.look_back-idx-1] = one_hot \n",
    "            else: #look_back以降なので気にせず詰め込めばよい\n",
    "                labels = self.labels.iloc[idx-self.look_back+1:idx+1].values\n",
    "                #print(self.labels.iloc[idx-self.look_back+1:idx+1].values)\n",
    "            #labels = self.labels.iloc[idx].values\n",
    "            return [data, labels.astype(\"int64\")]\n",
    "        else:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNBATCHSIZE = 200\n",
    "look_back=128\n",
    "# train_dataset = IronDataset(train_df, train[\"open_channels\"], training=True,look_back=look_back)\n",
    "train_dataset = subIronDataset(train[\"signal\"].values, train[\"open_channels\"], training=True,look_back=look_back)\n",
    "# test_dataset = IronDataset(test_df, training=False, look_back=look_back)\n",
    "test_dataset = subIronDataset(test[\"signal\"].values, training=False,look_back=look_back)\n",
    "# idx = [0,4999999,499999,500000,500001,4999996]\n",
    "# X, y = train_dataset[idx]\n",
    "# train_dataloader = DataLoader(train_dataset, NNBATCHSIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "# test_dataloader = DataLoader(test_dataset, NNBATCHSIZE, shuffle=False, num_workers=8, pin_memory=True)\n",
    "#-0.5805814\n",
    "train_dataloader = DataLoader(train_dataset, NNBATCHSIZE, shuffle=True, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h = (B, n_aux, T)\n",
    "# n_aux = 1\n",
    "# model = WaveNet(n_quantize=256, n_aux=n_aux, n_resch=128, n_skipch=128,\n",
    "#                  dilation_depth=9, dilation_repeat=2, kernel_size=2, upsampling_factor=0)\n",
    "# print(\"model.receptive_field:\",model.receptive_field)\n",
    "# h = torch.zeros((200, n_aux, look_back))\n",
    "# x,y = train_dataset[0]\n",
    "# print(x.shape, y.shape)\n",
    "# # print(x, y)\n",
    "# for x, y in train_dataloader:\n",
    "#     print(x.shape, y.shape)\n",
    "#     pred = model(x, h)\n",
    "#     print(pred.data.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred.contiguous().view(-1,11).shape)\n",
    "# print(y.contiguous().view(-1).shape)\n",
    "# a = pred.contiguous().view(-1,11)\n",
    "# # b = torch.LongTensor(y.contiguous().view(-1))\n",
    "# b = y.contiguous().view(-1)\n",
    "# print(a.shape, b.shape)\n",
    "# print(a)\n",
    "# print(b)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# loss = criterion(a,b)\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n",
    "        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n",
    "        self.counter, self.best_score = 0, None\n",
    "        self.is_maximize = is_maximize\n",
    "\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        model.load_state_dict(torch.load(self.checkpoint_path))\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None or \\\n",
    "                (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n",
    "            torch.save(model.state_dict(), self.checkpoint_path)\n",
    "            self.best_score, self.counter = score, 0\n",
    "            return 1\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return 2\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の方法\n",
    "\n",
    "* マイ時刻のクロスエントロピーを計算する方法（本家）  \n",
    "* 最終出力のみでクロスエントロピーを計算する（簡単）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\n"
    }
   ],
   "source": [
    "devise = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(devise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b09ef9f36c2049989f93882d18c681fa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "**********************************\nFolder : 0 Epoch : 0\nCurr learning_rate: 0.001000000\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4098e6e912de4949980859ba10783f22"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ede8bebfb268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m#loss_fn(model(input), target).backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m#optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"non pred\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work2/i_kuroyanagi/kaggle/liverpool-ion-switching/wavenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdil_sigmoid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdil_tanh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_1x1_sigmoid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_1x1_tanh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 self.skip_1x1[l], self.res_1x1[l])\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0mskip_connections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work2/i_kuroyanagi/kaggle/liverpool-ion-switching/wavenet.py\u001b[0m in \u001b[0;36m_residual_forward\u001b[0;34m(self, x, h, dil_sigmoid, dil_tanh, aux_1x1_sigmoid, aux_1x1_tanh, skip_1x1, res_1x1)\u001b[0m\n\u001b[1;32m    504\u001b[0m                           aux_1x1_sigmoid, aux_1x1_tanh, skip_1x1, res_1x1):\n\u001b[1;32m    505\u001b[0m         \u001b[0moutput_sigmoid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdil_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0moutput_tanh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdil_tanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0maux_output_sigmoid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux_1x1_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0maux_output_tanh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux_1x1_tanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work2/i_kuroyanagi/kaggle/liverpool-ion-switching/wavenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFloat\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/py36/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    206\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    207\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 208\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "n_aux = 1\n",
    "n_quantize = 256\n",
    "look_back = 256\n",
    "EPOCHS = 20\n",
    "NNBATCHSIZE = 4000\n",
    "lr = 1e-3\n",
    "h = torch.zeros((NNBATCHSIZE, n_aux, look_back)).to(devise)\n",
    "                \n",
    "oof_score = []\n",
    "for index,(train_index, val_index) in enumerate(kf.split(np.zeros((train.shape[0],1)), train[\"open_channels\"])):\n",
    "    train_dataset = subIronDataset(train[\"signal\"], train[\"open_channels\"], training=True, index=train_index, look_back=look_back)\n",
    "    train_dataloader = DataLoader(train_dataset, NNBATCHSIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    valid_dataset = subIronDataset(train[\"signal\"], train[\"open_channels\"], training=True, index=val_index, look_back=look_back)\n",
    "    valid_dataloader = DataLoader(valid_dataset, NNBATCHSIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    it = 0\n",
    "    \n",
    "    model = WaveNet(n_quantize=256, n_aux=n_aux, n_resch=64, n_skipch=128,\n",
    "                 dilation_depth=6, dilation_repeat=2, kernel_size=2, upsampling_factor=0)\n",
    "    model = model.to(devise)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10, is_maximize=True,\n",
    "                                   checkpoint_path=os.path.join(\"models\", \"checkpoint_fold_{}_iter_{}.pt\".format(index,it)))\n",
    "    cols = [\"loss\", \"F1\", \"val_loss\", \"val_F1\", \"lr\"]\n",
    "    results = pd.DataFrame(columns=cols)\n",
    "    weight = None#cal_weights()\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.2)\n",
    "    avg_train_losses, avg_valid_losses = [], []\n",
    "    \n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        print('**********************************')\n",
    "        print(\"Folder : {} Epoch : {}\".format(index, epoch))\n",
    "        print(\"Curr learning_rate: {:0.9f}\".format(optimizer.param_groups[0]['lr']))\n",
    "        train_losses, valid_losses = [], []\n",
    "        tr_loss_cls_item, val_loss_cls_item = [], []\n",
    "\n",
    "        model.train()  # prep model for training\n",
    "        train_preds, train_true = torch.Tensor([]).to(devise), torch.LongTensor([]).to(devise)\n",
    "        cnt = 0\n",
    "        for x, y in tqdm(train_dataloader):\n",
    "            x = x.to(devise)\n",
    "            y = y.to(devise)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            #loss_fn(model(input), target).backward()\n",
    "            #optimizer.zero_grad()\n",
    "            predictions = model(x, h)\n",
    "            if cnt == 0:\n",
    "                print(\"non pred\",predictions.shape)\n",
    "                print(\"non y   \",y.shape)\n",
    "            loss = criterion( predictions.contiguous().view(-1, 11), y.contiguous().view(-1))\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            \n",
    "            #schedular.step()\n",
    "            # record training lossa\n",
    "            train_losses.append(loss.item())\n",
    "            train_true = torch.cat([train_true, y.contiguous()[:,-1]], 0)\n",
    "            train_preds = torch.cat([train_preds, predictions.contiguous()[:,-1,:]], 0)\n",
    "            cnt += 1\n",
    "        model.eval()  # prep model for evaluation\n",
    "        #optimizer.swap_swa_sgd()\n",
    "        val_preds, val_true = torch.Tensor([]).to(devise), torch.LongTensor([]).to(devise)\n",
    "        print('EVALUATION')\n",
    "        with torch.no_grad():\n",
    "            for x, y in valid_dataloader:\n",
    "                x = x.to(devise)\n",
    "                y = y.to(devise)\n",
    "\n",
    "                predictions = model(x, h)\n",
    "                loss = criterion(predictions.contiguous().view(-1, 11), y.contiguous().view(-1))\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "                val_true = torch.cat([val_true, y.contiguous()[:,-1]], 0)\n",
    "                val_preds = torch.cat([val_preds, predictions.contiguous()[:,-1,:]], 0)\n",
    "\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        print(\"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n",
    "\n",
    "        train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1),\n",
    "                               labels=list(range(11)), average='macro')\n",
    "\n",
    "        val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1),\n",
    "                             labels=list(range(11)), average='macro')\n",
    "        tmp = pd.DataFrame([train_loss, train_score, valid_loss, val_score, optimizer.param_groups[0]['lr']],columns=cols)\n",
    "        results = pd.concat([results, tmp], axis=0)\n",
    "        results.to_csv('output/results_fold{}.csv'.format(index), index=False)\n",
    "        schedular.step(val_score)\n",
    "        print(\"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n",
    "        res = early_stopping(val_score, model)\n",
    "        #print('fres:', res)\n",
    "        if  res == 2:\n",
    "            print(\"Early Stopping\")\n",
    "            print('folder %d global best val max f1 model score %f' % (index, early_stopping.best_score))\n",
    "            break\n",
    "        elif res == 1:\n",
    "            print('save folder %d global val max f1 model score %f' % (index, val_score))\n",
    "    print('Folder {} finally best global max f1 score is {}'.format(index, early_stopping.best_score))\n",
    "    oof_score.append(round(early_stopping.best_score, 6))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = subIronDataset(test[\"signal\"], training=False)\n",
    "test_dataloader = DataLoader(test_dataset, NNBATCHSIZE, shuffle=False)\n",
    "model.eval()\n",
    "pred_list = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dataloader:\n",
    "        x = x.to(devise)\n",
    "        y = y.to(devise)\n",
    "\n",
    "        predictions = model(x, h)\n",
    "        pred_list.append(F.softmax(predictions.contiguous()[:,-1,:], dim=1).cpu().numpy()) # shape (512000, 11)\n",
    "    test_preds = np.vstack(pred_list) # shape [2000000, 11]\n",
    "    test_preds_all += test_preds\n",
    "print('all folder score is:%s'%str(oof_score))\n",
    "print('OOF mean score is: %f'% (sum(oof_score)/len(oof_score)))\n",
    "print('Generate submission.............')\n",
    "test_preds_all = test_preds_all / np.sum(test_preds_all, axis=1)[:, None]\n",
    "test_pred_frame = pd.DataFrame({'time': sample_submission['time'].astype(str),\n",
    "                                'open_channels': np.argmax(test_preds_all, axis=1)})\n",
    "test_pred_frame.to_csv(\"preds/wavenet_preds.csv\", index=False)\n",
    "print('over')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}