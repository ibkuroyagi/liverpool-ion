{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\n"
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wavenet import *\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import io\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%precision 3\n",
    "sys.path.append('../')\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "print(device)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "SEED = 42\n",
    "lr = 1e-3\n",
    "# if you need to reload to apply changeing function, you can use this \n",
    "%autoreload\n",
    "\n",
    "# from original_variables import B4_Z, corr_data4_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 0             1             2             3             4   \\\ncount  2.669400e+04  2.669400e+04  2.669400e+04  2.669400e+04  2.669400e+04   \nmean   1.235078e-15  6.941992e-16  2.555335e-17 -2.257212e-15 -5.706914e-16   \nstd    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \nmin   -2.058446e+00 -2.092422e+00 -1.997026e+00 -1.668652e+00 -1.801076e+00   \n25%   -4.699158e-01 -5.510449e-01 -6.145777e-01 -5.662558e-01 -5.554222e-01   \n50%   -1.825097e-01 -2.159564e-01 -2.651920e-01 -3.112324e-01 -2.892532e-01   \n75%    1.685148e-01  1.976680e-01  2.891797e-01  1.001024e-01  1.119641e-01   \nmax    7.763553e+00  6.010963e+00  7.205731e+00  5.621428e+00  6.675084e+00   \n\n                 5             6             7             8             9   \\\ncount  2.669400e+04  2.669400e+04  2.669400e+04  2.669400e+04  2.669400e+04   \nmean   1.358586e-15  4.331292e-15 -3.162227e-16  3.321935e-16  1.309609e-15   \nstd    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \nmin   -1.417576e+00 -1.677417e+00 -1.834140e+00 -1.924426e+00 -1.725664e+00   \n25%   -6.442780e-01 -7.347827e-01 -4.576305e-01 -5.569729e-01 -6.326598e-01   \n50%   -4.282878e-01 -4.167227e-01 -1.732174e-01 -2.140049e-01 -3.488928e-01   \n75%    2.672621e-01  5.303100e-01  1.593763e-01  2.261374e-01  2.402691e-01   \nmax    5.170766e+00  4.920170e+00  9.680590e+00  7.891894e+00  5.362618e+00   \n\n       ...            40            41            42            43  \\\ncount  ...  2.669400e+04  2.669400e+04  2.669400e+04  2.669400e+04   \nmean   ...  2.704396e-16  2.832163e-15  9.284382e-16 -2.326419e-15   \nstd    ...  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \nmin    ... -2.485807e+00 -3.058671e+00 -2.053440e+00 -1.930758e+00   \n25%    ... -6.266575e-01 -6.611715e-01 -5.483359e-01 -6.047582e-01   \n50%    ... -1.866651e-01 -8.023465e-02 -2.786751e-01 -2.992419e-01   \n75%    ...  3.724913e-01  5.500360e-01  1.023709e-01  1.977409e-01   \nmax    ...  6.646375e+00  7.876607e+00  7.057272e+00  6.019823e+00   \n\n                 44            45            46            47            48  \\\ncount  2.669400e+04  2.669400e+04  2.669400e+04  2.669400e+04  2.669400e+04   \nmean  -9.635741e-17 -3.918180e-16  1.128606e-15  1.925019e-15  2.235918e-17   \nstd    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \nmin   -2.249340e+00 -1.503422e+00 -2.394259e+00 -2.067178e+00 -2.644919e+00   \n25%   -6.229022e-01 -6.365820e-01 -6.211954e-01 -5.521250e-01 -6.392076e-01   \n50%   -2.030998e-01 -3.724305e-01 -1.637034e-01 -2.309297e-01 -1.913624e-01   \n75%    3.586932e-01  2.732392e-01  3.968476e-01  1.903251e-01  3.755492e-01   \nmax    6.873250e+00  5.308989e+00  9.428258e+00  6.686560e+00  6.168876e+00   \n\n                 49  \ncount  2.669400e+04  \nmean   1.618379e-16  \nstd    1.000000e+00  \nmin   -2.470368e+00  \n25%   -5.822687e-01  \n50%   -1.616123e-01  \n75%    3.571590e-01  \nmax    1.265253e+01  \n\n[8 rows x 50 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>40</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>...</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n      <td>2.669400e+04</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.235078e-15</td>\n      <td>6.941992e-16</td>\n      <td>2.555335e-17</td>\n      <td>-2.257212e-15</td>\n      <td>-5.706914e-16</td>\n      <td>1.358586e-15</td>\n      <td>4.331292e-15</td>\n      <td>-3.162227e-16</td>\n      <td>3.321935e-16</td>\n      <td>1.309609e-15</td>\n      <td>...</td>\n      <td>2.704396e-16</td>\n      <td>2.832163e-15</td>\n      <td>9.284382e-16</td>\n      <td>-2.326419e-15</td>\n      <td>-9.635741e-17</td>\n      <td>-3.918180e-16</td>\n      <td>1.128606e-15</td>\n      <td>1.925019e-15</td>\n      <td>2.235918e-17</td>\n      <td>1.618379e-16</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>...</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-2.058446e+00</td>\n      <td>-2.092422e+00</td>\n      <td>-1.997026e+00</td>\n      <td>-1.668652e+00</td>\n      <td>-1.801076e+00</td>\n      <td>-1.417576e+00</td>\n      <td>-1.677417e+00</td>\n      <td>-1.834140e+00</td>\n      <td>-1.924426e+00</td>\n      <td>-1.725664e+00</td>\n      <td>...</td>\n      <td>-2.485807e+00</td>\n      <td>-3.058671e+00</td>\n      <td>-2.053440e+00</td>\n      <td>-1.930758e+00</td>\n      <td>-2.249340e+00</td>\n      <td>-1.503422e+00</td>\n      <td>-2.394259e+00</td>\n      <td>-2.067178e+00</td>\n      <td>-2.644919e+00</td>\n      <td>-2.470368e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-4.699158e-01</td>\n      <td>-5.510449e-01</td>\n      <td>-6.145777e-01</td>\n      <td>-5.662558e-01</td>\n      <td>-5.554222e-01</td>\n      <td>-6.442780e-01</td>\n      <td>-7.347827e-01</td>\n      <td>-4.576305e-01</td>\n      <td>-5.569729e-01</td>\n      <td>-6.326598e-01</td>\n      <td>...</td>\n      <td>-6.266575e-01</td>\n      <td>-6.611715e-01</td>\n      <td>-5.483359e-01</td>\n      <td>-6.047582e-01</td>\n      <td>-6.229022e-01</td>\n      <td>-6.365820e-01</td>\n      <td>-6.211954e-01</td>\n      <td>-5.521250e-01</td>\n      <td>-6.392076e-01</td>\n      <td>-5.822687e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-1.825097e-01</td>\n      <td>-2.159564e-01</td>\n      <td>-2.651920e-01</td>\n      <td>-3.112324e-01</td>\n      <td>-2.892532e-01</td>\n      <td>-4.282878e-01</td>\n      <td>-4.167227e-01</td>\n      <td>-1.732174e-01</td>\n      <td>-2.140049e-01</td>\n      <td>-3.488928e-01</td>\n      <td>...</td>\n      <td>-1.866651e-01</td>\n      <td>-8.023465e-02</td>\n      <td>-2.786751e-01</td>\n      <td>-2.992419e-01</td>\n      <td>-2.030998e-01</td>\n      <td>-3.724305e-01</td>\n      <td>-1.637034e-01</td>\n      <td>-2.309297e-01</td>\n      <td>-1.913624e-01</td>\n      <td>-1.616123e-01</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.685148e-01</td>\n      <td>1.976680e-01</td>\n      <td>2.891797e-01</td>\n      <td>1.001024e-01</td>\n      <td>1.119641e-01</td>\n      <td>2.672621e-01</td>\n      <td>5.303100e-01</td>\n      <td>1.593763e-01</td>\n      <td>2.261374e-01</td>\n      <td>2.402691e-01</td>\n      <td>...</td>\n      <td>3.724913e-01</td>\n      <td>5.500360e-01</td>\n      <td>1.023709e-01</td>\n      <td>1.977409e-01</td>\n      <td>3.586932e-01</td>\n      <td>2.732392e-01</td>\n      <td>3.968476e-01</td>\n      <td>1.903251e-01</td>\n      <td>3.755492e-01</td>\n      <td>3.571590e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.763553e+00</td>\n      <td>6.010963e+00</td>\n      <td>7.205731e+00</td>\n      <td>5.621428e+00</td>\n      <td>6.675084e+00</td>\n      <td>5.170766e+00</td>\n      <td>4.920170e+00</td>\n      <td>9.680590e+00</td>\n      <td>7.891894e+00</td>\n      <td>5.362618e+00</td>\n      <td>...</td>\n      <td>6.646375e+00</td>\n      <td>7.876607e+00</td>\n      <td>7.057272e+00</td>\n      <td>6.019823e+00</td>\n      <td>6.873250e+00</td>\n      <td>5.308989e+00</td>\n      <td>9.428258e+00</td>\n      <td>6.686560e+00</td>\n      <td>6.168876e+00</td>\n      <td>1.265253e+01</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 50 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "B_Z = pd.read_csv('B4_Z.csv', index_col=0).T\n",
    "B_Z.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(133, 49, 100) (66, 49, 100) (67, 49, 100)\n(133, 100) (66, 100) (67, 100)\n"
    }
   ],
   "source": [
    "look_back = 100\n",
    "time_conv_frame = 64\n",
    "fold = 0\n",
    "\n",
    "X_idx = np.arange(49)\n",
    "n_feature = len(X_idx)\n",
    "X = B_Z.iloc[:,X_idx]\n",
    "X_train, X_valid, X_test = train_val_test(X, look_back, fold)\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)\n",
    "\n",
    "y_idx = np.array([49])\n",
    "y = pd.DataFrame(B_Z.iloc[:,y_idx])\n",
    "y_train, y_valid, y_test = train_val_test(y, look_back, fold)\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)\n",
    "\n",
    "train_dataset = conv2dDataset(X_train, y_train)\n",
    "valid_dataset = conv2dDataset(X_valid, y_valid)\n",
    "test_dataset = conv2dDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "kernel size:65, time_conv_frame:65, padding:(0, 32)\n_kernel size:2, padding:1\nx:torch.Size([8, 1, 49, 100])\nx_hat:torch.Size([8, 32, 1, 100])\n_x_hat:torch.Size([8, 16, 101])\n"
    }
   ],
   "source": [
    "in_channels = 1\n",
    "out_channels = 32\n",
    "dilation = 1\n",
    "\n",
    "if time_conv_frame % 2 == 0:\n",
    "    time_conv_frame += 1\n",
    "padding = (0, (time_conv_frame - 1) // 2)\n",
    "kernel_size = (n_feature, time_conv_frame)\n",
    "print(f\"kernel size:{time_conv_frame}, time_conv_frame:{time_conv_frame}, padding:{padding}\")\n",
    "conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding).to(device)\n",
    "\n",
    "_kernel_size = 2\n",
    "dilation = 1\n",
    "padding = (_kernel_size - 1) * dilation\n",
    "print(f\"_kernel size:{_kernel_size}, padding:{padding}\")\n",
    "causal = nn.Conv1d(in_channels=out_channels, out_channels=16, kernel_size=_kernel_size, padding=padding).to(device)\n",
    "for x, y in train_dataloader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    print(f\"x:{x.shape}\")\n",
    "    x_hat = conv2d(x)\n",
    "    print(f\"x_hat:{x_hat.shape}\")\n",
    "    x_hat = x_hat.squeeze()\n",
    "    _x_hat = causal(x_hat)\n",
    "    break\n",
    "\n",
    "print(f\"_x_hat:{_x_hat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x:torch.Size([8, 1, 49, 100]), y:torch.Size([8, 100]), y_hat:torch.Size([8, 100])\n"
    }
   ],
   "source": [
    "time_conv_frame = 64\n",
    "model = WaveNet_conv2d(n_resch=512, n_skipch=256, n_feature=n_feature,\n",
    "                 dilation_depth=5, dilation_repeat=3, kernel_size=2,\n",
    "                 in_channels=in_channels, out_channels=out_channels, time_conv_frame=time_conv_frame)\n",
    "model.to(device)\n",
    "for x, y in train_dataloader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)  \n",
    "    y_hat = model(x)\n",
    "    break\n",
    "print(f\"x:{x.shape}, y:{y.shape}, y_hat:{y_hat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-0536f9bdc483>, line 62)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-0536f9bdc483>\"\u001b[0;36m, line \u001b[0;32m62\u001b[0m\n\u001b[0;31m    if b_size != n_feature\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train_wavenet_conv2d(model, train_dataset, valid_dataset, test_dataset, device=\"cuda\", \n",
    "                      BATCH_SIZE=16, EPOCHS=100, lr=1e-3,\n",
    "                     model_path=\"ex.pth\", log_path=\"ex.csv\"):\n",
    "    \"\"\"\n",
    "    Training WaveNet and predict test data\n",
    "    calculate local to local per one cell by WaveNet structure.\n",
    "    In this function, finally, you can get across cluster's correlation matrix in test data. \n",
    "    \n",
    "    detail of rule of calculation\n",
    "        input : X (Tensor): Doudle tensor variable with the shape (B, in_channels, n_feature, T).\n",
    "        output : series y (batch, time)\n",
    "        loss function : MSE.\n",
    "        metric : correlation which axis is time course. \n",
    "    Paramns\n",
    "    ----------------------------------\n",
    "    dataloader :dataset \n",
    "        {X (n_feature, look_back), y (look_back)}\n",
    "    model_path : str, default \"ex.pth\"\n",
    "        path where you save the model\n",
    "    log_path : str, default \"ex.csv\"\n",
    "        path where you save the log\n",
    "    \n",
    "    Returns\n",
    "    ----------------------------------\n",
    "    test_corr : float\n",
    "        correlation of test_true and test_pred\n",
    "    test_preds : ndarray\n",
    "        output of WaveNeet from test data\n",
    "    log_df : DataFrame\n",
    "        log of this model [\"loss\", \"corr\", \"val_loss\", \"val_corr\", \"lr\"]\n",
    "    \"\"\"\n",
    "    print(model_path)\n",
    "    print(log_path)\n",
    "    train_dataloader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, BATCH_SIZE, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    \n",
    "    model = model.to(device)\n",
    "    early_stopping = EarlyStopping(patience=20, device=device, checkpoint_path=model_path)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.8)\n",
    "    avg_train_losses, avg_valid_losses = [], []\n",
    "    cols = [\"loss\", \"corr\", \"val_loss\", \"val_corr\", \"lr\"]\n",
    "    log_df = pd.DataFrame(columns=cols)\n",
    "    cnt = 0\n",
    "\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        #seed_everything(SEED)\n",
    "        train_losses, valid_losses = [], []\n",
    "        tr_loss_cls_item, val_loss_cls_item = [], []\n",
    "        model.train()  # prep model for training\n",
    "        train_preds, train_true = torch.Tensor([]).to(device), torch.Tensor([]).to(device)\n",
    "        cnt = 0\n",
    "        for x, y in train_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            b_size = x.size(1)\n",
    "            if len(b_size) == 2:\n",
    "                x = x.unsqueeze(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x)\n",
    "            #loss = criterion( predictions[:, model.receptive_field:].contiguous().view(-1, 256), y[:, model.receptive\n",
    "            loss = criterion(predictions.contiguous().view(-1), y.contiguous().view(-1))\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "            train_true = torch.cat([train_true, y.contiguous().view(-1)], 0)\n",
    "            train_preds = torch.cat([train_preds, predictions.contiguous().view(-1)], 0)\n",
    "            cnt += 1\n",
    "        model.eval()  # prep model for evaluation\n",
    "        val_preds, val_true = torch.Tensor([]).to(device), torch.Tensor([]).to(device)\n",
    "        with torch.no_grad():\n",
    "            for x, y in valid_dataloader:\n",
    "                x = x.to(device)#.reshape(-1, n_feature, look_back)\n",
    "                y = y.to(device)\n",
    "                predictions = model(x)\n",
    "                #loss = criterion(predictions[:, model.receptive_field:].contiguous().view(-1, 256), y[:, model.receptive_field:].contiguous().view(-1))\n",
    "                loss = criterion(predictions.contiguous().view(-1), y.contiguous().view(-1))\n",
    "                valid_losses.append(loss.item())\n",
    "                val_true = torch.cat([val_true, y.contiguous().view(-1)], 0)\n",
    "                val_preds = torch.cat([val_preds, predictions.contiguous().view(-1)], 0)\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        train_corr = np.corrcoef(x=train_true.cpu().numpy(), y=train_preds.cpu().detach().numpy())[0, 1]\n",
    "        valid_corr = np.corrcoef(x=val_true.cpu().numpy(), y=val_preds.cpu().detach().numpy())[0, 1]\n",
    "        print(\"Epoch : {}, lr:{:0.7f}, \".format(epoch,optimizer.param_groups[0]['lr']) +\\\n",
    "            \"loss: {:0.5f}, corr: {:0.5f}, val_loss: {:0.5f}, val_corr: {:.5f}\".format(train_loss, train_corr, valid_loss, valid_corr))\n",
    "        schedular.step(valid_corr)\n",
    "        \n",
    "        #logを保存する\n",
    "        tmp = pd.DataFrame([[train_loss, train_corr, valid_loss, valid_corr, optimizer.param_groups[0]['lr']]],columns=cols)\n",
    "        log_df = pd.concat([log_df, tmp], axis=0).reset_index(drop=True)\n",
    "        log_df.to_csv(log_path, index=False)\n",
    "        if train_corr >= 0.5:\n",
    "            res = early_stopping(valid_corr, model, mode=\"max\", epoch=epoch)\n",
    "            cnt = 1\n",
    "            if  res == 2:\n",
    "                print(\"Early Stopping\")\n",
    "                print('folder %d global best val max corr %f' % (fold, early_stopping.best_score))\n",
    "                break\n",
    "        if (epoch == EPOCHS - 1) & (cnt == 0):\n",
    "            res = early_stopping(valid_corr, model, mode=\"max\", epoch=epoch)\n",
    "\n",
    "    test_preds, test_true = torch.Tensor([]).to(device), torch.Tensor([]).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            predictions = model(x)\n",
    "            test_true = torch.cat([test_true, y.reshape(-1)], 0)\n",
    "            test_preds = torch.cat([test_preds, predictions.reshape(-1,)], 0)\n",
    "    test_true = test_true.cpu().numpy()\n",
    "    test_preds = test_preds.cpu().detach().numpy()\n",
    "    test_corr = np.corrcoef(x=test_true, y=test_preds)[0, 1]\n",
    "    print(f\" best Epoch:{early_stopping.epoch:}, test_corr:{test_corr:.4f}\")\n",
    "    return test_corr, test_preds, log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "out_channels = 32\n",
    "n_feature = 49\n",
    "time_conv_frame = 64\n",
    "model = WaveNet_conv2d(n_resch=512, n_skipch=256, n_feature=n_feature,\n",
    "                 dilation_depth=5, dilation_repeat=3, kernel_size=2,\n",
    "                 in_channels=in_channels, out_channels=out_channels, time_conv_frame=time_conv_frame)\n",
    "model.to(device)\n",
    "test_corr, test_preds, log_df = train_wavenet_conv2d(model, train_dataset, valid_dataset, test_dataset, device=\"cpu\", \n",
    "                      BATCH_SIZE=16, EPOCHS=100, lr=1e-3,\n",
    "                     model_path=\"ex.pth\", log_path=\"ex.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 0  1]\n [ 2  3]\n [ 4  5]\n [ 6  7]\n [ 8  9]\n [10 11]\n [12 13]\n [14 15]\n [16 17]\n [18 19]]\n[1 1 1 1 1 1 1 1 1 1]\n"
    }
   ],
   "source": [
    "a = np.arange(20).reshape(10, 2)\n",
    "print(a)\n",
    "print(a.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "a = (1,2,3)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bitpy36virtualenv1282f2306817482c86ec5e4cdfa22dfd",
   "display_name": "Python 3.6.8 64-bit ('py36': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}